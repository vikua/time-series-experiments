{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from time_series_experiments import utils\n",
    "from time_series_experiments import baseline\n",
    "from time_series_experiments import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0xC0FFEE\n",
    "fdw = 24\n",
    "fw = 12\n",
    "\n",
    "air_df = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = air_df['Passengers'].values\n",
    "\n",
    "x_train_idx, y_train_idx, x_test_idx, y_test_idx = utils.train_test_split_index(\n",
    "    y.shape[0], fdw, fw, test_size=0.1, random_seed=seed\n",
    ")\n",
    "\n",
    "x_train = np.expand_dims(y[x_train_idx], axis=-1)\n",
    "y_train = y[y_train_idx]\n",
    "x_test = np.expand_dims(y[x_test_idx], axis=-1)\n",
    "y_test = y[y_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.54480824984842"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model = baseline.LatestNaiveBaseline(fw=fw)\n",
    "y_pred_baseline = baseline_model.predict(x_test)\n",
    "utils.rmse(y_test, np.squeeze(y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99 samples\n",
      "Epoch 1/100\n",
      "99/99 [==============================] - 2s 21ms/sample - loss: 1.6511\n",
      "Epoch 2/100\n",
      "99/99 [==============================] - 0s 235us/sample - loss: 0.7011\n",
      "Epoch 3/100\n",
      "99/99 [==============================] - 0s 282us/sample - loss: 0.4051\n",
      "Epoch 4/100\n",
      "99/99 [==============================] - 0s 274us/sample - loss: 0.2323\n",
      "Epoch 5/100\n",
      "99/99 [==============================] - 0s 273us/sample - loss: 0.2182\n",
      "Epoch 6/100\n",
      "99/99 [==============================] - 0s 239us/sample - loss: 0.2429\n",
      "Epoch 7/100\n",
      "99/99 [==============================] - 0s 248us/sample - loss: 0.1943\n",
      "Epoch 8/100\n",
      "99/99 [==============================] - 0s 247us/sample - loss: 0.1912\n",
      "Epoch 9/100\n",
      "99/99 [==============================] - 0s 245us/sample - loss: 0.1770\n",
      "Epoch 10/100\n",
      "99/99 [==============================] - 0s 236us/sample - loss: 0.1897\n",
      "Epoch 11/100\n",
      "99/99 [==============================] - 0s 241us/sample - loss: 0.1790\n",
      "Epoch 12/100\n",
      "99/99 [==============================] - 0s 248us/sample - loss: 0.1700\n",
      "Epoch 13/100\n",
      "99/99 [==============================] - 0s 252us/sample - loss: 0.1838\n",
      "Epoch 14/100\n",
      "99/99 [==============================] - 0s 256us/sample - loss: 0.1759\n",
      "Epoch 15/100\n",
      "99/99 [==============================] - 0s 252us/sample - loss: 0.1694\n",
      "Epoch 16/100\n",
      "99/99 [==============================] - 0s 234us/sample - loss: 0.1738\n",
      "Epoch 17/100\n",
      "99/99 [==============================] - 0s 238us/sample - loss: 0.1722\n",
      "Epoch 18/100\n",
      "99/99 [==============================] - 0s 238us/sample - loss: 0.1674\n",
      "Epoch 19/100\n",
      "99/99 [==============================] - 0s 235us/sample - loss: 0.1656\n",
      "Epoch 20/100\n",
      "99/99 [==============================] - 0s 258us/sample - loss: 0.1656\n",
      "Epoch 21/100\n",
      "99/99 [==============================] - 0s 261us/sample - loss: 0.1608\n",
      "Epoch 22/100\n",
      "99/99 [==============================] - 0s 252us/sample - loss: 0.1572\n",
      "Epoch 23/100\n",
      "99/99 [==============================] - 0s 231us/sample - loss: 0.1544\n",
      "Epoch 24/100\n",
      "99/99 [==============================] - 0s 260us/sample - loss: 0.1468\n",
      "Epoch 25/100\n",
      "99/99 [==============================] - 0s 307us/sample - loss: 0.1425\n",
      "Epoch 26/100\n",
      "99/99 [==============================] - 0s 285us/sample - loss: 0.1355\n",
      "Epoch 27/100\n",
      "99/99 [==============================] - 0s 278us/sample - loss: 0.1281\n",
      "Epoch 28/100\n",
      "99/99 [==============================] - 0s 321us/sample - loss: 0.1197\n",
      "Epoch 29/100\n",
      "99/99 [==============================] - 0s 307us/sample - loss: 0.1138\n",
      "Epoch 30/100\n",
      "99/99 [==============================] - 0s 286us/sample - loss: 0.1111\n",
      "Epoch 31/100\n",
      "99/99 [==============================] - 0s 294us/sample - loss: 0.1053\n",
      "Epoch 32/100\n",
      "99/99 [==============================] - 0s 283us/sample - loss: 0.1000\n",
      "Epoch 33/100\n",
      "99/99 [==============================] - 0s 279us/sample - loss: 0.1009\n",
      "Epoch 34/100\n",
      "99/99 [==============================] - 0s 283us/sample - loss: 0.1018\n",
      "Epoch 35/100\n",
      "99/99 [==============================] - 0s 321us/sample - loss: 0.1000\n",
      "Epoch 36/100\n",
      "99/99 [==============================] - 0s 318us/sample - loss: 0.0965\n",
      "Epoch 37/100\n",
      "99/99 [==============================] - 0s 325us/sample - loss: 0.0925\n",
      "Epoch 38/100\n",
      "99/99 [==============================] - 0s 315us/sample - loss: 0.0876\n",
      "Epoch 39/100\n",
      "99/99 [==============================] - 0s 325us/sample - loss: 0.0846\n",
      "Epoch 40/100\n",
      "99/99 [==============================] - 0s 329us/sample - loss: 0.0832\n",
      "Epoch 41/100\n",
      "99/99 [==============================] - 0s 303us/sample - loss: 0.0816\n",
      "Epoch 42/100\n",
      "99/99 [==============================] - 0s 283us/sample - loss: 0.0846\n",
      "Epoch 43/100\n",
      "99/99 [==============================] - 0s 268us/sample - loss: 0.0847\n",
      "Epoch 44/100\n",
      "99/99 [==============================] - 0s 284us/sample - loss: 0.0839\n",
      "Epoch 45/100\n",
      "99/99 [==============================] - 0s 280us/sample - loss: 0.0828\n",
      "Epoch 46/100\n",
      "99/99 [==============================] - 0s 251us/sample - loss: 0.0811\n",
      "Epoch 47/100\n",
      "99/99 [==============================] - 0s 256us/sample - loss: 0.0795\n",
      "Epoch 48/100\n",
      "99/99 [==============================] - 0s 279us/sample - loss: 0.0778\n",
      "Epoch 49/100\n",
      "99/99 [==============================] - 0s 248us/sample - loss: 0.0735\n",
      "Epoch 50/100\n",
      "99/99 [==============================] - 0s 255us/sample - loss: 0.0719\n",
      "Epoch 51/100\n",
      "99/99 [==============================] - 0s 250us/sample - loss: 0.0722\n",
      "Epoch 52/100\n",
      "99/99 [==============================] - 0s 246us/sample - loss: 0.0702\n",
      "Epoch 53/100\n",
      "99/99 [==============================] - 0s 253us/sample - loss: 0.0700\n",
      "Epoch 54/100\n",
      "99/99 [==============================] - 0s 242us/sample - loss: 0.0770\n",
      "Epoch 55/100\n",
      "99/99 [==============================] - 0s 250us/sample - loss: 0.0777\n",
      "Epoch 56/100\n",
      "99/99 [==============================] - 0s 262us/sample - loss: 0.0680\n",
      "Epoch 57/100\n",
      "99/99 [==============================] - 0s 249us/sample - loss: 0.0708\n",
      "Epoch 58/100\n",
      "99/99 [==============================] - 0s 246us/sample - loss: 0.0725\n",
      "Epoch 59/100\n",
      "99/99 [==============================] - 0s 255us/sample - loss: 0.0678\n",
      "Epoch 60/100\n",
      "99/99 [==============================] - 0s 244us/sample - loss: 0.0674\n",
      "Epoch 61/100\n",
      "99/99 [==============================] - 0s 246us/sample - loss: 0.0656\n",
      "Epoch 62/100\n",
      "99/99 [==============================] - 0s 234us/sample - loss: 0.0770\n",
      "Epoch 63/100\n",
      "99/99 [==============================] - 0s 243us/sample - loss: 0.0860\n",
      "Epoch 64/100\n",
      "99/99 [==============================] - 0s 268us/sample - loss: 0.0733\n",
      "Epoch 65/100\n",
      "99/99 [==============================] - 0s 237us/sample - loss: 0.0653\n",
      "Epoch 66/100\n",
      "99/99 [==============================] - 0s 237us/sample - loss: 0.0749\n",
      "Epoch 67/100\n",
      "99/99 [==============================] - 0s 250us/sample - loss: 0.0744\n",
      "Epoch 68/100\n",
      "99/99 [==============================] - 0s 226us/sample - loss: 0.0706\n",
      "Epoch 69/100\n",
      "99/99 [==============================] - 0s 245us/sample - loss: 0.0670\n",
      "Epoch 70/100\n",
      "99/99 [==============================] - 0s 238us/sample - loss: 0.0635\n",
      "Epoch 71/100\n",
      "99/99 [==============================] - 0s 246us/sample - loss: 0.0643\n",
      "Epoch 72/100\n",
      "99/99 [==============================] - 0s 267us/sample - loss: 0.0628\n",
      "Epoch 73/100\n",
      "99/99 [==============================] - 0s 258us/sample - loss: 0.0618\n",
      "Epoch 74/100\n",
      "99/99 [==============================] - 0s 236us/sample - loss: 0.0652\n",
      "Epoch 75/100\n",
      "99/99 [==============================] - 0s 235us/sample - loss: 0.0681\n",
      "Epoch 76/100\n",
      "99/99 [==============================] - 0s 239us/sample - loss: 0.0619\n",
      "Epoch 77/100\n",
      "99/99 [==============================] - 0s 241us/sample - loss: 0.0683\n",
      "Epoch 78/100\n",
      "99/99 [==============================] - 0s 240us/sample - loss: 0.0614\n",
      "Epoch 79/100\n",
      "99/99 [==============================] - 0s 248us/sample - loss: 0.0624\n",
      "Epoch 80/100\n",
      "99/99 [==============================] - 0s 241us/sample - loss: 0.0645\n",
      "Epoch 81/100\n",
      "99/99 [==============================] - 0s 256us/sample - loss: 0.0636\n",
      "Epoch 82/100\n",
      "99/99 [==============================] - 0s 248us/sample - loss: 0.0645\n",
      "Epoch 83/100\n",
      "99/99 [==============================] - 0s 241us/sample - loss: 0.0716\n",
      "Epoch 84/100\n",
      "99/99 [==============================] - 0s 244us/sample - loss: 0.0593\n",
      "Epoch 85/100\n",
      "99/99 [==============================] - 0s 260us/sample - loss: 0.0662\n",
      "Epoch 86/100\n",
      "99/99 [==============================] - 0s 243us/sample - loss: 0.0659\n",
      "Epoch 87/100\n",
      "99/99 [==============================] - 0s 244us/sample - loss: 0.0585\n",
      "Epoch 88/100\n",
      "99/99 [==============================] - 0s 275us/sample - loss: 0.0673\n",
      "Epoch 89/100\n",
      "99/99 [==============================] - 0s 270us/sample - loss: 0.0761\n",
      "Epoch 90/100\n",
      "99/99 [==============================] - 0s 300us/sample - loss: 0.0728\n",
      "Epoch 91/100\n",
      "99/99 [==============================] - 0s 261us/sample - loss: 0.0588\n",
      "Epoch 92/100\n",
      "99/99 [==============================] - 0s 270us/sample - loss: 0.0655\n",
      "Epoch 93/100\n",
      "99/99 [==============================] - 0s 234us/sample - loss: 0.0644\n",
      "Epoch 94/100\n",
      "99/99 [==============================] - 0s 241us/sample - loss: 0.0619\n",
      "Epoch 95/100\n",
      "99/99 [==============================] - 0s 250us/sample - loss: 0.0571\n",
      "Epoch 96/100\n",
      "99/99 [==============================] - 0s 236us/sample - loss: 0.0580\n",
      "Epoch 97/100\n",
      "99/99 [==============================] - 0s 239us/sample - loss: 0.0562\n",
      "Epoch 98/100\n",
      "99/99 [==============================] - 0s 252us/sample - loss: 0.0566\n",
      "Epoch 99/100\n",
      "99/99 [==============================] - 0s 252us/sample - loss: 0.0648\n",
      "Epoch 100/100\n",
      "99/99 [==============================] - 0s 250us/sample - loss: 0.0626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1556ae210>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0.01, 0.99))\n",
    "\n",
    "xtr, ytr = x_train, y_train \n",
    "# xtr, ytr = np.log(xtr), np.log(ytr)\n",
    "scaler.fit(np.reshape(xtr, (-1, 1)))\n",
    "original_x_shape = xtr.shape\n",
    "original_y_shape = ytr.shape\n",
    "xtr = scaler.transform(np.reshape(xtr, (-1, 1)))\n",
    "ytr = scaler.transform(np.reshape(ytr, (-1, 1)))\n",
    "xtr = np.reshape(xtr, original_x_shape)\n",
    "ytr = np.reshape(ytr, original_y_shape)\n",
    "\n",
    "transformer_model = transformer.Transformer(\n",
    "    num_layers=1,\n",
    "    attention_dim=4,\n",
    "    num_heads=4,\n",
    "    linear_kernel_initializer=utils.get_initializer(\"glorot_uniform\", seed),\n",
    "    attention_kernel_initializer=utils.get_initializer(\"glorot_uniform\", seed),\n",
    "    pwffn_kernel_initializer=utils.get_initializer(\"glorot_uniform\", seed),\n",
    "    output_kernel_initializer=utils.get_initializer(\"glorot_uniform\", seed),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    optimizer=keras.optimizers.Adam(0.01),\n",
    "    loss=keras.losses.MeanAbsoluteError(),\n",
    ")\n",
    "transformer_model.fit(xtr, ytr, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _make_execution_function.<locals>.distributed_function at 0x156f545f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _make_execution_function.<locals>.distributed_function at 0x156f545f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function _make_execution_function.<locals>.distributed_function at 0x156f545f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function _make_execution_function.<locals>.distributed_function at 0x156f545f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function _make_execution_function.<locals>.distributed_function at 0x156f545f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function _make_execution_function.<locals>.distributed_function at 0x156f545f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x156f545f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x156f545f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('RMSE', 64.66266019105187, 'MASE', 0.7181447631203906)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt = x_test\n",
    "# xt = np.log(xt)\n",
    "original_x_shape = xt.shape\n",
    "xt = scaler.transform(np.reshape(xt, (-1, 1)))\n",
    "xt = np.reshape(xt, original_x_shape)\n",
    "\n",
    "y_pred, _ = transformer_model.predict(xt)\n",
    "y_pred = scaler.inverse_transform(y_pred)\n",
    "# y_pred = np.exp(y_pred)\n",
    "\n",
    "'RMSE', utils.rmse(y_test, y_pred), 'MASE', utils.mase(y_test, y_pred, np.squeeze(y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
